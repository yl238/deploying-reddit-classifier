{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataset\n",
    "1. Scrape Reddit data using Praw (Python wrapper around Reddit API)\n",
    "2. Prepare data into the correct format for labelling.\n",
    "\n",
    "Note: Need to have a `praw.ini` file in this directory for authentication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date(created_at):\n",
    "    return datetime.fromtimestamp(created)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Post(object):\n",
    "    def __init__(self, session):\n",
    "        self.title = session.title\n",
    "        self.score = session.score\n",
    "        self.url = session.url\n",
    "        self.num_comments = session.num_comments\n",
    "        self.created_at = get_date(session.created)\n",
    "        self.body = session.selftext\n",
    "\n",
    "    def __dict__(self):\n",
    "        return {'title': self.title, 'score': self.score, 'num_comments': self.num_comments,\n",
    "                'created_at': self.created_at, 'body': self.body, 'url': self.url}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RedditScraper(object):\n",
    "    def __init__(self, subreddit):\n",
    "        self.reddit = praw.Reddit('UserTesting')\n",
    "        self.subreddit = self.reddit.subreddit(subreddit)\n",
    "    \n",
    "    def scrape(self, limit):\n",
    "        new_posts = self.subreddit.new(limit=limit)\n",
    "        for session in new_posts:\n",
    "            yield Post(session)\n",
    "\n",
    "    def get_data(self, limit):\n",
    "        data = []\n",
    "        for post in self.scrape(limit):\n",
    "            data.append(post.__dict__())\n",
    "        df = pd.DataFrame.from_records(data)\n",
    "        df['text'] = df['title'].fillna('') + ' ' + df['body'].fillna('')\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper = RedditScraper('usertesting')\n",
    "df = scraper.get_data(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('dataset/reddit_scrape.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
